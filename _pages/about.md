---
layout: about
title: About
permalink: /
subtitle: he-ming.xia AT connect.polyu.hk

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular

selected_papers: true # includes a list of papers marked as "selected={true}"

announcements:
  enabled: true # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder
---

I am Heming Xia (Â§èÈπ§Êòé), a Ph.D. student in the [NLP Group](https://polyunlp.github.io/) at The Hong Kong Polytechnic University, supervised by Prof. [Wenjie Li](https://www4.comp.polyu.edu.hk/~cswjli/). I earned my master's degree from the [MOE Key Lab of Computational Linguistics](https://icl.pku.edu.cn/) at Peking University, advised by Prof. [Zhifang Sui](https://cs.pku.edu.cn/info/1226/2014.htm). Prior to that, I completed my bachelor's degree from the [School of Physics](http://english.phy.pku.edu.cn/info/1017/5151.htm) at Peking University. I have also worked as a Research Intern at the [NLC Group @ Microsoft Research Asia](https://www.microsoft.com/en-us/research/group/natural-language-computing/), where I had the privilege of collaborating with Dr. [Tao Ge](https://scholar.google.com/citations?user=LYbs7Q8AAAAJ&hl=en). Currently, I am an intern at [SEA AI Lab](https://sail.sea.com/), working closely with Dr. [Cunxiao Du](https://nonvolatilememory.github.io/). For more details, please refer to my [CV](https://hemingkx.github.io/assets/pdf/CV.pdf).

üì¨ *I am open to collaborating with highly motivated students on research related to (but not limited to) the topics below. If interested, please feel free to reach out via email.*

# Research

My research focuses on efficient and effective NLP, with the goal of making LLMs faster, more scalable, and broadly applicable. Specifically, my work centers on the following directions:

- **Speculative Decoding:** Exploring inference acceleration techniques that maintain output fidelity. This includes our pioneering work on Speculative Decoding [[EMNLP'23-findings](https://aclanthology.org/2023.findings-emnlp.257/), [ICLR'25](https://openreview.net/forum?id=EKJhH5D5wA)] and the first comprehensive survey [[ACL'24-findings](https://aclanthology.org/2024.findings-acl.456/)] on this emerging paradigm.
- **Efficient Reasoning:** Developing advanced algorithms to enhance the efficiency of reasoning models, spanning efficient training strategies, inference acceleration [[EMNLP'25](https://arxiv.org/abs/2502.12067)], and novel reasoning representations such as latent CoT [[arXiv'25](https://arxiv.org/abs/2505.16782)].
- **Applications (Efficiency + X):** I am interested in how efficiency-oriented techniques can benefit broader applications, with recent focus on tool-augmented agents and multimodal models [[EMNLP'25](https://arxiv.org/abs/2508.16201)].

In addition, I am actively working on tool learning [e.g., [EMNLP'24](https://aclanthology.org/2024.emnlp-main.856/), [ACL'25-findings](https://aclanthology.org/2025.findings-acl.1107/)] and vision-language understanding [e.g., [ACL'22](https://aclanthology.org/2022.acl-long.66/), [EMNLP'23-findings](https://aclanthology.org/2023.findings-emnlp.133/), [EMNLP'25-findings](https://arxiv.org/abs/2502.13925)].
